{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import *\n",
    "import dill as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gap(g, h, x_val, y_val, x_val_predictions):\n",
    "    \"\"\"\n",
    "    Evaluates the hypothesis model on the group and compares it to the current model's perfromance on the group.\n",
    "    \"\"\"\n",
    "    loss_fn = sk.metrics.mean_squared_error\n",
    "\n",
    "    indices_val = g(x_val).astype('bool')\n",
    "    if indices_val.sum() <= 1:\n",
    "        return False\n",
    "\n",
    "    model_error_val = loss_fn(y_val[indices_val], x_val_predictions[indices_val])\n",
    "    hypothesis_preds_val = h(x_val[indices_val])\n",
    "    hypothesis_error_val = loss_fn(y_val[indices_val], hypothesis_preds_val)\n",
    "\n",
    "    return model_error_val, hypothesis_error_val, indices_val.sum()/len(indices_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pkl(g, h, suffix):\n",
    "    with open(f'g-{suffix}.pkl', 'wb') as file:\n",
    "        pkl.dump(g, file)\n",
    "\n",
    "    with open(f'h-{suffix}.pkl', 'wb') as file:\n",
    "        pkl.dump(h, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (340134, 21)\n"
     ]
    }
   ],
   "source": [
    "x_train = pd.read_csv('training_data.csv') \n",
    "y_train = np.genfromtxt('training_labels.csv', delimiter=',', dtype = float)\n",
    "print(f'x_train.shape: {x_train.shape}')\n",
    "\n",
    "y_mean = y_train.mean()\n",
    "y_std = y_train.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_predictions.shape: (340134,)\n"
     ]
    }
   ],
   "source": [
    "prediction_path = 'training_predictions.csv'\n",
    "x_predictions = np.loadtxt(prediction_path, delimiter=\",\", dtype=str).astype('float64')\n",
    "print(f'x_predictions.shape: {x_predictions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    sk.neural_network.MLPClassifier(max_iter=100),\n",
    "    sk.ensemble.AdaBoostClassifier(),\n",
    "    sk.ensemble.GradientBoostingClassifier(),\n",
    "    sk.naive_bayes.BernoulliNB(),\n",
    "]\n",
    "\n",
    "def get_best_classifier(X, y):\n",
    "    best_score = float('inf')\n",
    "    best_classifier = None\n",
    "    for clf in classifiers:\n",
    "        clf.fit(X, y)\n",
    "        score = clf.score(X, y)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_classifier = clf\n",
    "    \n",
    "    print(best_classifier)\n",
    "    return best_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressors = [\n",
    "    sk.neural_network.MLPRegressor(max_iter=1000, verbose=True),\n",
    "    sk.ensemble.AdaBoostRegressor(),\n",
    "    sk.ensemble.GradientBoostingRegressor(),\n",
    "    sk.linear_model.ElasticNet(),\n",
    "    sk.linear_model.TweedieRegressor(),\n",
    "]\n",
    "\n",
    "def get_best_regressor(X, y):\n",
    "    best_score = float('inf')\n",
    "    best_regressor = None\n",
    "    for reg in regressors:\n",
    "        print(reg)\n",
    "        reg.fit(X, y)\n",
    "        score = reg.score(X, y)\n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_regressor = reg\n",
    "    \n",
    "    print(best_regressor)\n",
    "    return best_regressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_clustering(X, y, i):\n",
    "    \"\"\"\n",
    "    Defines groups based on K-Means clustering on the one-hot features.\n",
    "    \"\"\"\n",
    "    enc = sk.preprocessing.OneHotEncoder().fit(X)\n",
    "    X = enc.transform(X).toarray()\n",
    "    clustering = sk.cluster.KMeans(n_clusters=24, n_init='auto', random_state=42).fit(X)\n",
    "\n",
    "    def g(X):\n",
    "        X = enc.transform(X).toarray()\n",
    "        predictions = clustering.predict(X)\n",
    "        return predictions == i\n",
    "\n",
    "    return g, enc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_meta(X, y, x_train_predictions):\n",
    "    \"\"\"\n",
    "    Defines group as the top 10% worst errors based on predictions lifted from training_predictions.csv.\n",
    "    \"\"\"\n",
    "    errors = np.abs(x_train_predictions - y)\n",
    "    threshold = np.sort(errors)[-int(len(errors)*0.1)]\n",
    "    g_y = errors >= threshold\n",
    "    print(g_y.sum())\n",
    "\n",
    "    enc = sk.preprocessing.OneHotEncoder().fit(X)\n",
    "    # poly = sk.preprocessing.PolynomialFeatures(2).fit(enc.transform(X))\n",
    "    def transform(X):\n",
    "        X = enc.transform(X).toarray() if enc else X\n",
    "        # X = poly.transform(X) if poly else X\n",
    "        return X\n",
    "\n",
    "    X = transform(X)\n",
    "    g_clf = get_best_classifier(X, g_y)\n",
    "    print(g_clf.predict(X).sum())\n",
    "    \n",
    "    def g(X):\n",
    "        X = transform(X)\n",
    "        return g_clf.predict(X)\n",
    "    return g, transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h(X, y, g, transform=None):\n",
    "    indices = g(X)\n",
    "    print(indices.sum())\n",
    "\n",
    "    X = transform(X) if transform else X\n",
    "    clf = get_best_regressor(X[indices], (y[indices] - y_mean) / y_std)\n",
    "\n",
    "    def h(X):\n",
    "        X = transform(X) if transform else X\n",
    "        return clf.predict(X) * y_std + y_mean\n",
    "\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28911\n",
      "BernoulliNB()\n",
      "14591\n",
      "14591\n",
      "MLPRegressor(max_iter=10000, verbose=True)\n",
      "Iteration 1, loss = 0.47147533\n",
      "Iteration 2, loss = 0.40131034\n",
      "Iteration 3, loss = 0.38948811\n",
      "Iteration 4, loss = 0.38268502\n",
      "Iteration 5, loss = 0.37480038\n",
      "Iteration 6, loss = 0.36860001\n",
      "Iteration 7, loss = 0.35830349\n",
      "Iteration 8, loss = 0.34848020\n",
      "Iteration 9, loss = 0.33781585\n",
      "Iteration 10, loss = 0.32679664\n",
      "Iteration 11, loss = 0.31457134\n",
      "Iteration 12, loss = 0.30125425\n",
      "Iteration 13, loss = 0.28877046\n",
      "Iteration 14, loss = 0.27783167\n",
      "Iteration 15, loss = 0.26474479\n",
      "Iteration 16, loss = 0.25286565\n",
      "Iteration 17, loss = 0.24200414\n",
      "Iteration 18, loss = 0.23378465\n",
      "Iteration 19, loss = 0.22500981\n",
      "Iteration 20, loss = 0.21510652\n",
      "Iteration 21, loss = 0.20813317\n",
      "Iteration 22, loss = 0.20075035\n",
      "Iteration 23, loss = 0.19287889\n",
      "Iteration 24, loss = 0.18732835\n",
      "Iteration 25, loss = 0.18226378\n",
      "Iteration 26, loss = 0.17523313\n",
      "Iteration 27, loss = 0.17246938\n",
      "Iteration 28, loss = 0.16479091\n",
      "Iteration 29, loss = 0.16005802\n",
      "Iteration 30, loss = 0.15548324\n",
      "Iteration 31, loss = 0.15216049\n",
      "Iteration 32, loss = 0.14818107\n",
      "Iteration 33, loss = 0.14406060\n",
      "Iteration 34, loss = 0.14211746\n",
      "Iteration 35, loss = 0.13733428\n",
      "Iteration 36, loss = 0.13284675\n",
      "Iteration 37, loss = 0.13014112\n",
      "Iteration 38, loss = 0.12903184\n",
      "Iteration 39, loss = 0.12555143\n",
      "Iteration 40, loss = 0.12156450\n",
      "Iteration 41, loss = 0.11934731\n",
      "Iteration 42, loss = 0.11667178\n",
      "Iteration 43, loss = 0.11716922\n",
      "Iteration 44, loss = 0.11295611\n",
      "Iteration 45, loss = 0.11086184\n",
      "Iteration 46, loss = 0.10861766\n",
      "Iteration 47, loss = 0.10576590\n",
      "Iteration 48, loss = 0.10383385\n",
      "Iteration 49, loss = 0.10183568\n",
      "Iteration 50, loss = 0.10013753\n",
      "Iteration 51, loss = 0.09792775\n",
      "Iteration 52, loss = 0.09687329\n",
      "Iteration 53, loss = 0.09359246\n",
      "Iteration 54, loss = 0.09260495\n",
      "Iteration 55, loss = 0.09086439\n",
      "Iteration 56, loss = 0.08904780\n",
      "Iteration 57, loss = 0.08998882\n",
      "Iteration 58, loss = 0.08750671\n",
      "Iteration 59, loss = 0.08542371\n",
      "Iteration 60, loss = 0.08390267\n",
      "Iteration 61, loss = 0.08281181\n",
      "Iteration 62, loss = 0.08192604\n",
      "Iteration 63, loss = 0.07980694\n",
      "Iteration 64, loss = 0.07999719\n",
      "Iteration 65, loss = 0.07986124\n",
      "Iteration 66, loss = 0.07696707\n",
      "Iteration 67, loss = 0.07574667\n",
      "Iteration 68, loss = 0.07382824\n",
      "Iteration 69, loss = 0.07325805\n",
      "Iteration 70, loss = 0.07236133\n",
      "Iteration 71, loss = 0.07062833\n",
      "Iteration 72, loss = 0.06941581\n",
      "Iteration 73, loss = 0.07036221\n",
      "Iteration 74, loss = 0.06720653\n",
      "Iteration 75, loss = 0.06844611\n",
      "Iteration 76, loss = 0.06558076\n",
      "Iteration 77, loss = 0.06700376\n",
      "Iteration 78, loss = 0.06542127\n",
      "Iteration 79, loss = 0.06539463\n",
      "Iteration 80, loss = 0.06286012\n",
      "Iteration 81, loss = 0.06181187\n",
      "Iteration 82, loss = 0.06085295\n",
      "Iteration 83, loss = 0.06110154\n",
      "Iteration 84, loss = 0.06042996\n",
      "Iteration 85, loss = 0.05854435\n",
      "Iteration 86, loss = 0.05819070\n",
      "Iteration 87, loss = 0.05871603\n",
      "Iteration 88, loss = 0.05651973\n",
      "Iteration 89, loss = 0.05670166\n",
      "Iteration 90, loss = 0.05605160\n",
      "Iteration 91, loss = 0.05593592\n",
      "Iteration 92, loss = 0.05403016\n",
      "Iteration 93, loss = 0.05399363\n",
      "Iteration 94, loss = 0.05252150\n",
      "Iteration 95, loss = 0.05233767\n",
      "Iteration 96, loss = 0.05181551\n",
      "Iteration 97, loss = 0.05107951\n",
      "Iteration 98, loss = 0.05087539\n",
      "Iteration 99, loss = 0.05024492\n",
      "Iteration 100, loss = 0.04938634\n",
      "Iteration 101, loss = 0.04847387\n",
      "Iteration 102, loss = 0.04866414\n",
      "Iteration 103, loss = 0.04729025\n",
      "Iteration 104, loss = 0.04632561\n",
      "Iteration 105, loss = 0.04764732\n",
      "Iteration 106, loss = 0.04635900\n",
      "Iteration 107, loss = 0.04597145\n",
      "Iteration 108, loss = 0.04468870\n",
      "Iteration 109, loss = 0.04439771\n",
      "Iteration 110, loss = 0.04519643\n",
      "Iteration 111, loss = 0.04464709\n",
      "Iteration 112, loss = 0.04343209\n",
      "Iteration 113, loss = 0.04255234\n",
      "Iteration 114, loss = 0.04282008\n",
      "Iteration 115, loss = 0.04326651\n",
      "Iteration 116, loss = 0.04146680\n",
      "Iteration 117, loss = 0.04163974\n",
      "Iteration 118, loss = 0.04175607\n",
      "Iteration 119, loss = 0.04122900\n",
      "Iteration 120, loss = 0.04028667\n",
      "Iteration 121, loss = 0.04024409\n",
      "Iteration 122, loss = 0.04111322\n",
      "Iteration 123, loss = 0.04002692\n",
      "Iteration 124, loss = 0.03949338\n",
      "Iteration 125, loss = 0.03784436\n",
      "Iteration 126, loss = 0.03766214\n",
      "Iteration 127, loss = 0.03731634\n",
      "Iteration 128, loss = 0.03830734\n",
      "Iteration 129, loss = 0.03810559\n",
      "Iteration 130, loss = 0.03758371\n",
      "Iteration 131, loss = 0.03737882\n",
      "Iteration 132, loss = 0.03739464\n",
      "Iteration 133, loss = 0.03577909\n",
      "Iteration 134, loss = 0.03479326\n",
      "Iteration 135, loss = 0.03481275\n",
      "Iteration 136, loss = 0.03375573\n",
      "Iteration 137, loss = 0.03442952\n",
      "Iteration 138, loss = 0.03521653\n",
      "Iteration 139, loss = 0.03511051\n",
      "Iteration 140, loss = 0.03335448\n",
      "Iteration 141, loss = 0.03351138\n",
      "Iteration 142, loss = 0.03472303\n",
      "Iteration 143, loss = 0.03318031\n",
      "Iteration 144, loss = 0.03303976\n",
      "Iteration 145, loss = 0.03198273\n",
      "Iteration 146, loss = 0.03201033\n",
      "Iteration 147, loss = 0.03215195\n",
      "Iteration 148, loss = 0.03134822\n",
      "Iteration 149, loss = 0.03171813\n",
      "Iteration 150, loss = 0.03036025\n",
      "Iteration 151, loss = 0.03163729\n",
      "Iteration 152, loss = 0.03012951\n",
      "Iteration 153, loss = 0.03036890\n",
      "Iteration 154, loss = 0.02939498\n",
      "Iteration 155, loss = 0.02962135\n",
      "Iteration 156, loss = 0.03051111\n",
      "Iteration 157, loss = 0.02919320\n",
      "Iteration 158, loss = 0.02906851\n",
      "Iteration 159, loss = 0.03051296\n",
      "Iteration 160, loss = 0.02883225\n",
      "Iteration 161, loss = 0.02824755\n",
      "Iteration 162, loss = 0.02776985\n",
      "Iteration 163, loss = 0.02732510\n",
      "Iteration 164, loss = 0.02691959\n",
      "Iteration 165, loss = 0.02759076\n",
      "Iteration 166, loss = 0.02673768\n",
      "Iteration 167, loss = 0.02676615\n",
      "Iteration 168, loss = 0.02690973\n",
      "Iteration 169, loss = 0.02743168\n",
      "Iteration 170, loss = 0.02603619\n",
      "Iteration 171, loss = 0.02528808\n",
      "Iteration 172, loss = 0.02514939\n",
      "Iteration 173, loss = 0.02546237\n",
      "Iteration 174, loss = 0.02530413\n",
      "Iteration 175, loss = 0.02537120\n",
      "Iteration 176, loss = 0.02560968\n",
      "Iteration 177, loss = 0.02525510\n",
      "Iteration 178, loss = 0.02459874\n",
      "Iteration 179, loss = 0.02523073\n",
      "Iteration 180, loss = 0.02420646\n",
      "Iteration 181, loss = 0.02355681\n",
      "Iteration 182, loss = 0.02486419\n",
      "Iteration 183, loss = 0.02363221\n",
      "Iteration 184, loss = 0.02356057\n",
      "Iteration 185, loss = 0.02434339\n",
      "Iteration 186, loss = 0.02329238\n",
      "Iteration 187, loss = 0.02354854\n",
      "Iteration 188, loss = 0.02504483\n",
      "Iteration 189, loss = 0.02333087\n",
      "Iteration 190, loss = 0.02213835\n",
      "Iteration 191, loss = 0.02237832\n",
      "Iteration 192, loss = 0.02201126\n",
      "Iteration 193, loss = 0.02188477\n",
      "Iteration 194, loss = 0.02186606\n",
      "Iteration 195, loss = 0.02165851\n",
      "Iteration 196, loss = 0.02221446\n",
      "Iteration 197, loss = 0.02174393\n",
      "Iteration 198, loss = 0.02174103\n",
      "Iteration 199, loss = 0.02305986\n",
      "Iteration 200, loss = 0.02141001\n",
      "Iteration 201, loss = 0.02170209\n",
      "Iteration 202, loss = 0.02138483\n",
      "Iteration 203, loss = 0.02079671\n",
      "Iteration 204, loss = 0.02037235\n",
      "Iteration 205, loss = 0.01970203\n",
      "Iteration 206, loss = 0.01968309\n",
      "Iteration 207, loss = 0.02049676\n",
      "Iteration 208, loss = 0.01971467\n",
      "Iteration 209, loss = 0.02006881\n",
      "Iteration 210, loss = 0.02055102\n",
      "Iteration 211, loss = 0.02004628\n",
      "Iteration 212, loss = 0.02023844\n",
      "Iteration 213, loss = 0.01964840\n",
      "Iteration 214, loss = 0.01940164\n",
      "Iteration 215, loss = 0.01937501\n",
      "Iteration 216, loss = 0.01884643\n",
      "Iteration 217, loss = 0.01899337\n",
      "Iteration 218, loss = 0.01906545\n",
      "Iteration 219, loss = 0.01978281\n",
      "Iteration 220, loss = 0.01903742\n",
      "Iteration 221, loss = 0.01879542\n",
      "Iteration 222, loss = 0.01800242\n",
      "Iteration 223, loss = 0.01801294\n",
      "Iteration 224, loss = 0.01785791\n",
      "Iteration 225, loss = 0.01834225\n",
      "Iteration 226, loss = 0.01839743\n",
      "Iteration 227, loss = 0.01906367\n",
      "Iteration 228, loss = 0.01803912\n",
      "Iteration 229, loss = 0.01770527\n",
      "Iteration 230, loss = 0.01857433\n",
      "Iteration 231, loss = 0.01726149\n",
      "Iteration 232, loss = 0.01749746\n",
      "Iteration 233, loss = 0.01778918\n",
      "Iteration 234, loss = 0.01742960\n",
      "Iteration 235, loss = 0.01669566\n",
      "Iteration 236, loss = 0.01797883\n",
      "Iteration 237, loss = 0.01802128\n",
      "Iteration 238, loss = 0.01719216\n",
      "Iteration 239, loss = 0.01690162\n",
      "Iteration 240, loss = 0.01696491\n",
      "Iteration 241, loss = 0.01710363\n",
      "Iteration 242, loss = 0.01705815\n",
      "Iteration 243, loss = 0.01698928\n",
      "Iteration 244, loss = 0.01607475\n",
      "Iteration 245, loss = 0.01680134\n",
      "Iteration 246, loss = 0.01635422\n",
      "Iteration 247, loss = 0.01654578\n",
      "Iteration 248, loss = 0.01625471\n",
      "Iteration 249, loss = 0.01555934\n",
      "Iteration 250, loss = 0.01589333\n",
      "Iteration 251, loss = 0.01585993\n",
      "Iteration 252, loss = 0.01641415\n",
      "Iteration 253, loss = 0.01589256\n",
      "Iteration 254, loss = 0.01509925\n",
      "Iteration 255, loss = 0.01524665\n",
      "Iteration 256, loss = 0.01506794\n",
      "Iteration 257, loss = 0.01524553\n",
      "Iteration 258, loss = 0.01563266\n",
      "Iteration 259, loss = 0.01565808\n",
      "Iteration 260, loss = 0.01556620\n",
      "Iteration 261, loss = 0.01545059\n",
      "Iteration 262, loss = 0.01506733\n",
      "Iteration 263, loss = 0.01570041\n",
      "Iteration 264, loss = 0.01576116\n",
      "Iteration 265, loss = 0.01552650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "AdaBoostRegressor()\n",
      "GradientBoostingRegressor()\n",
      "ElasticNet()\n",
      "TweedieRegressor()\n",
      "ElasticNet()\n"
     ]
    }
   ],
   "source": [
    "x_train_subset, x_val, y_train_subset, y_val, x_predictions_subset, x_predictions_val = sk.model_selection.train_test_split(x_train, y_train, x_predictions, test_size=.15, random_state=42)\n",
    "\n",
    "get_g = get_g_meta\n",
    "g, transform = get_g(x_train_subset, y_train_subset, x_predictions_subset)\n",
    "h = get_h(x_train_subset, y_train_subset, g, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14591 in group from x_train_subset\n",
      "\n",
      "Baseline error: 399555865.28285867\n",
      "Hypothesis group error: 564862844.4618349\n",
      "Error gap: -165306979.17897624\n",
      "Group fraction in x_val: 0.05105740773407028\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Found {g(x_train_subset).sum()} in group from x_train_subset')\n",
    "print()\n",
    "\n",
    "model_error, h_error, fraction = get_gap(g, h, x_val, y_val, x_predictions_val)\n",
    "print(f'Baseline error: {model_error}')\n",
    "print(f'Hypothesis group error: {h_error}')\n",
    "print(f'Error gap: {model_error - h_error}')\n",
    "print(f'Group fraction in x_val: {fraction}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34013\n",
      "14993\n",
      "14993\n"
     ]
    }
   ],
   "source": [
    "g, enc = get_g(x_train, y_train, x_predictions)\n",
    "h = get_h(x_train, y_train, g, enc)\n",
    "save_pkl(g, h, f'meta')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
